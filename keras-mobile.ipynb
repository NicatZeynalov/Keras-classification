{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile Price Classification using Keras\n",
    "\n",
    "\n",
    "In this notebook, I will make my first neural network(ANN) using keras framework. The data is about mobile phones of various companies and consist of  features of a mobile phone(eg:- RAM,Internal Memory etc) and its selling price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries and Data processing\n",
    "The dataset consists of 20 features and we need to predict the price range in which phone lies. These ranges are divided into 4 classes. Before feeding data to our neural network we need it in a specific way so we need to process it accordingly. The preprocessing of data depends on the type of data. Here we will  handle image dataset. Let’s start the coding part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\NicatZeynalov\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\NicatZeynalov\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\NicatZeynalov\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\NicatZeynalov\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\NicatZeynalov\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\NicatZeynalov\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#libraries import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>756</td>\n",
       "      <td>2549</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1021</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>905</td>\n",
       "      <td>1988</td>\n",
       "      <td>2631</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1263</td>\n",
       "      <td>1716</td>\n",
       "      <td>2603</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>615</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1216</td>\n",
       "      <td>1786</td>\n",
       "      <td>2769</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1821</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1208</td>\n",
       "      <td>1212</td>\n",
       "      <td>1411</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
       "0            842     0          2.2         0   1       0           7    0.6   \n",
       "1           1021     1          0.5         1   0       1          53    0.7   \n",
       "2            563     1          0.5         1   2       1          41    0.9   \n",
       "3            615     1          2.5         0   0       0          10    0.8   \n",
       "4           1821     1          1.2         0  13       1          44    0.6   \n",
       "\n",
       "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
       "0        188        2  ...         20       756  2549     9     7         19   \n",
       "1        136        3  ...        905      1988  2631    17     3          7   \n",
       "2        145        5  ...       1263      1716  2603    11     2          9   \n",
       "3        131        6  ...       1216      1786  2769    16     8         11   \n",
       "4        141        2  ...       1208      1212  1411     8     2         15   \n",
       "\n",
       "   three_g  touch_screen  wifi  price_range  \n",
       "0        0             0     1            1  \n",
       "1        1             1     0            2  \n",
       "2        1             1     0            2  \n",
       "3        1             0     0            2  \n",
       "4        1             1     0            1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset import\n",
    "dataset = pd.read_csv('train.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset\n",
    "X = dataset.iloc[:,:20]\n",
    "y = dataset.iloc[:, 20:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to normalize the data. Normalization is a technique used to change the values of an array to a common scale, without distorting differences in the ranges of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "sc =  StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to one hot encode the classes. One hot encoding is a process to convert integer classes into binary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NicatZeynalov\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# OneHotEncoder\n",
    "one = OneHotEncoder()\n",
    "y = one.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now our dataset is processed and ready to feed in the neural network.__\n",
    "\n",
    "Generally, it is better to split data into training and testing data. Training data is the data on which we will train our neural network. Test data is used to check our trained neural network. This data is totally new for our neural network and if the neural network performs well on this dataset, it shows that there is no overfitting. Read more about this here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will split our dataset into training and testing. Training data will have 90% samples and test data will have 10% samples. This is specified by the test_size argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Neural Network\n",
    "\n",
    "In our dataset, the input is of 20 values and output is of 4 values. So the input and output layer is of 20 and 4 dimensions respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NicatZeynalov\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Building neural network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(16, input_dim = 20, activation = 'relu'))\n",
    "model.add(Dense(12, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training model\n",
    "\n",
    "We will specify the input data-> X_train, labels-> y_train, number of epochs(iterations), and batch size. It returns the history of model training. History consists of model accuracy and losses after each epoch. We will visualize it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.0551\n",
      "Epoch 2/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0541\n",
      "Epoch 3/100\n",
      "1800/1800 [==============================] - 0s 60us/step - loss: 0.0575\n",
      "Epoch 4/100\n",
      "1800/1800 [==============================] - 0s 51us/step - loss: 0.0533\n",
      "Epoch 5/100\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.0528\n",
      "Epoch 6/100\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.0519\n",
      "Epoch 7/100\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.0508\n",
      "Epoch 8/100\n",
      "1800/1800 [==============================] - 0s 49us/step - loss: 0.0493\n",
      "Epoch 9/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0492\n",
      "Epoch 10/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0483\n",
      "Epoch 11/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.0476\n",
      "Epoch 12/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0468\n",
      "Epoch 13/100\n",
      "1800/1800 [==============================] - 0s 25us/step - loss: 0.0464\n",
      "Epoch 14/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0459\n",
      "Epoch 15/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0448\n",
      "Epoch 16/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.0434\n",
      "Epoch 17/100\n",
      "1800/1800 [==============================] - 0s 52us/step - loss: 0.0435\n",
      "Epoch 18/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0434\n",
      "Epoch 19/100\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.0426\n",
      "Epoch 20/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0422\n",
      "Epoch 21/100\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.0414\n",
      "Epoch 22/100\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.0407\n",
      "Epoch 23/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.0402\n",
      "Epoch 24/100\n",
      "1800/1800 [==============================] - 0s 55us/step - loss: 0.0388\n",
      "Epoch 25/100\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.0383\n",
      "Epoch 26/100\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.0385\n",
      "Epoch 27/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.0380\n",
      "Epoch 28/100\n",
      "1800/1800 [==============================] - ETA: 0s - loss: 0.036 - 0s 38us/step - loss: 0.0367\n",
      "Epoch 29/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0365\n",
      "Epoch 30/100\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.0357\n",
      "Epoch 31/100\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.0350\n",
      "Epoch 32/100\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.0346\n",
      "Epoch 33/100\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.0346\n",
      "Epoch 34/100\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.0334\n",
      "Epoch 35/100\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.0323\n",
      "Epoch 36/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0324\n",
      "Epoch 37/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0317\n",
      "Epoch 38/100\n",
      "1800/1800 [==============================] - 0s 46us/step - loss: 0.0318\n",
      "Epoch 39/100\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.0326\n",
      "Epoch 40/100\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.0311\n",
      "Epoch 41/100\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.0306\n",
      "Epoch 42/100\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.0296\n",
      "Epoch 43/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0293\n",
      "Epoch 44/100\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.0280\n",
      "Epoch 45/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.0299\n",
      "Epoch 46/100\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.0281\n",
      "Epoch 47/100\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.0273\n",
      "Epoch 48/100\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.0270\n",
      "Epoch 49/100\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.0261\n",
      "Epoch 50/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0273\n",
      "Epoch 51/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0256\n",
      "Epoch 52/100\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.0249\n",
      "Epoch 53/100\n",
      "1800/1800 [==============================] - 0s 47us/step - loss: 0.0243\n",
      "Epoch 54/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0238\n",
      "Epoch 55/100\n",
      "1800/1800 [==============================] - 0s 50us/step - loss: 0.0243\n",
      "Epoch 56/100\n",
      "1800/1800 [==============================] - 0s 84us/step - loss: 0.0237\n",
      "Epoch 57/100\n",
      "1800/1800 [==============================] - 0s 51us/step - loss: 0.0240\n",
      "Epoch 58/100\n",
      "1800/1800 [==============================] - 0s 45us/step - loss: 0.0227\n",
      "Epoch 59/100\n",
      "1800/1800 [==============================] - 0s 51us/step - loss: 0.0223\n",
      "Epoch 60/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0228\n",
      "Epoch 61/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0223\n",
      "Epoch 62/100\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.0215\n",
      "Epoch 63/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.0211\n",
      "Epoch 64/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.0208\n",
      "Epoch 65/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0215\n",
      "Epoch 66/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.0201\n",
      "Epoch 67/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0199\n",
      "Epoch 68/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0194\n",
      "Epoch 69/100\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.0197\n",
      "Epoch 70/100\n",
      "1800/1800 [==============================] - 0s 43us/step - loss: 0.0192\n",
      "Epoch 71/100\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.0201\n",
      "Epoch 72/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0197\n",
      "Epoch 73/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.0184\n",
      "Epoch 74/100\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.0180\n",
      "Epoch 75/100\n",
      "1800/1800 [==============================] - 0s 47us/step - loss: 0.0174\n",
      "Epoch 76/100\n",
      "1800/1800 [==============================] - 0s 50us/step - loss: 0.0171\n",
      "Epoch 77/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0173\n",
      "Epoch 78/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.0167\n",
      "Epoch 79/100\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.0165\n",
      "Epoch 80/100\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.0160\n",
      "Epoch 81/100\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.0165\n",
      "Epoch 82/100\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.0161\n",
      "Epoch 83/100\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.0151\n",
      "Epoch 84/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.0151\n",
      "Epoch 85/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0146\n",
      "Epoch 86/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0146\n",
      "Epoch 87/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0141\n",
      "Epoch 88/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.0140\n",
      "Epoch 89/100\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.0138\n",
      "Epoch 90/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.0136\n",
      "Epoch 91/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0135\n",
      "Epoch 92/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0131\n",
      "Epoch 93/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0128\n",
      "Epoch 94/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.0129\n",
      "Epoch 95/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.0132\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.0127\n",
      "Epoch 97/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0121\n",
      "Epoch 98/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.0118\n",
      "Epoch 99/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.0120\n",
      "Epoch 100/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0119\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs = 100, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.42019582e-01 5.79804070e-02 4.24564870e-13 3.19895280e-16]\n",
      " [3.78914469e-33 4.12623551e-16 2.13963780e-09 1.00000000e+00]\n",
      " [8.16772163e-01 1.83227822e-01 1.73683151e-11 2.80377880e-13]\n",
      " [9.99788344e-01 2.11611565e-04 2.08135832e-19 6.37216323e-21]\n",
      " [4.84794115e-11 2.41767298e-02 9.75747347e-01 7.59922841e-05]\n",
      " [7.50475228e-01 2.49524727e-01 2.14354623e-09 1.03480002e-11]\n",
      " [4.24883241e-16 1.30013883e-04 5.47533214e-01 4.52336758e-01]\n",
      " [9.99972463e-01 2.75016264e-05 1.12128969e-20 9.72165421e-23]\n",
      " [1.59578022e-07 5.48363626e-01 4.51506495e-01 1.29708860e-04]\n",
      " [1.43061287e-03 9.98569250e-01 9.49782475e-08 7.67727062e-11]\n",
      " [9.98557031e-01 1.44301506e-03 4.11754759e-16 2.80370346e-18]\n",
      " [1.01880360e-32 9.69662244e-14 9.04656588e-07 9.99999046e-01]\n",
      " [9.99845266e-01 1.54730747e-04 1.41315577e-18 1.50957780e-20]\n",
      " [5.66153670e-23 3.93765420e-09 2.25530079e-04 9.99774516e-01]\n",
      " [1.74922584e-15 6.21056592e-04 9.90228772e-01 9.15019121e-03]\n",
      " [1.62689127e-02 9.83730674e-01 3.70461805e-07 4.34607572e-10]\n",
      " [1.25831150e-08 2.30360162e-02 9.75451946e-01 1.51206064e-03]\n",
      " [1.58972058e-09 9.96335864e-01 3.66417365e-03 3.91460411e-08]\n",
      " [1.24895610e-04 9.99871254e-01 3.80116603e-06 1.58373148e-09]\n",
      " [5.27322628e-02 9.47267711e-01 6.39947872e-09 4.99276922e-12]\n",
      " [1.54085457e-31 1.26926464e-13 9.10948813e-07 9.99999046e-01]\n",
      " [2.62860507e-23 6.99934555e-10 6.86013591e-05 9.99931335e-01]\n",
      " [9.39652085e-01 6.03479184e-02 1.50836094e-10 1.63843949e-12]\n",
      " [9.10188258e-01 8.98117945e-02 8.87965465e-11 1.11730818e-12]\n",
      " [4.85629457e-08 8.54550719e-01 1.45447940e-01 1.47563424e-06]\n",
      " [9.99987960e-01 1.20643008e-05 1.40178336e-22 1.41262545e-24]\n",
      " [3.55681100e-22 3.82196873e-07 3.56966518e-02 9.64302897e-01]\n",
      " [1.30749584e-04 9.99800503e-01 6.87785432e-05 2.15313278e-09]\n",
      " [6.83615830e-30 2.07536557e-14 2.57885002e-08 1.00000000e+00]\n",
      " [9.97894585e-01 2.10549589e-03 3.20373855e-18 5.38236125e-21]\n",
      " [9.99979138e-01 2.08270158e-05 5.84105027e-21 1.05037933e-21]\n",
      " [5.11256144e-14 3.61927203e-04 9.99467552e-01 1.70524043e-04]\n",
      " [6.91559101e-14 5.04480326e-04 7.73307145e-01 2.26188391e-01]\n",
      " [5.15308857e-01 4.84691173e-01 5.34137803e-11 7.14966958e-14]\n",
      " [4.15778959e-05 9.99958277e-01 6.87296122e-08 7.35414334e-12]\n",
      " [3.93552750e-01 6.06447220e-01 1.46399814e-09 3.30485921e-12]\n",
      " [3.50315153e-04 9.99111593e-01 5.37904969e-04 2.53978527e-07]\n",
      " [6.14918827e-06 9.12765503e-01 8.71482044e-02 8.00728740e-05]\n",
      " [1.61848864e-15 5.33248713e-05 8.37233245e-01 1.62713438e-01]\n",
      " [1.07312891e-15 1.49610464e-03 9.97979343e-01 5.24515926e-04]\n",
      " [9.99981165e-01 1.87791102e-05 1.75727299e-22 7.44810912e-25]\n",
      " [1.74978153e-36 6.51211238e-17 5.39559952e-10 1.00000000e+00]\n",
      " [1.13915473e-06 9.77344334e-01 2.26525236e-02 2.01171815e-06]\n",
      " [9.43725001e-17 1.29695429e-04 9.99762118e-01 1.08177286e-04]\n",
      " [8.94718937e-14 8.17535503e-04 9.97304201e-01 1.87815935e-03]\n",
      " [9.83172953e-16 1.33939568e-04 9.40694571e-01 5.91715239e-02]\n",
      " [9.96505605e-07 9.83078539e-01 1.69193260e-02 1.07032281e-06]\n",
      " [5.11843155e-06 9.99746859e-01 2.47961289e-04 2.27123920e-09]\n",
      " [9.99999881e-01 1.15566877e-07 3.86640957e-28 6.46987577e-30]\n",
      " [1.25938268e-12 1.96486269e-03 9.97800171e-01 2.34865845e-04]\n",
      " [6.59950264e-03 9.93400574e-01 3.84339144e-10 8.40718825e-14]\n",
      " [1.43225270e-22 7.66492869e-10 4.65679150e-05 9.99953389e-01]\n",
      " [6.80597787e-28 1.99670966e-12 5.75447984e-07 9.99999404e-01]\n",
      " [6.89087987e-09 7.76502430e-01 2.23488361e-01 9.19740523e-06]\n",
      " [6.55131415e-02 9.33760881e-01 7.19039585e-04 6.88049568e-06]\n",
      " [9.97120023e-01 2.87999329e-03 1.03656339e-16 2.76827495e-19]\n",
      " [6.76552022e-13 1.89633425e-02 9.81032073e-01 4.53992379e-06]\n",
      " [9.99749005e-01 2.51050806e-04 3.77194624e-16 8.03145391e-18]\n",
      " [9.99999523e-01 4.49062952e-07 1.20008707e-25 1.04749250e-27]\n",
      " [2.43882135e-01 7.56117821e-01 4.04186977e-08 2.39988390e-10]\n",
      " [9.70671058e-01 2.93290056e-02 4.92891535e-13 2.45450412e-15]\n",
      " [9.99998450e-01 1.52208088e-06 5.60359702e-25 6.02484196e-26]\n",
      " [9.99996185e-01 3.82806093e-06 3.14711744e-25 3.96296640e-27]\n",
      " [2.49687208e-15 7.23845791e-04 9.99267519e-01 8.60144610e-06]\n",
      " [5.27976839e-26 7.79221354e-10 3.88194865e-04 9.99611795e-01]\n",
      " [1.04127107e-09 5.14010012e-01 4.85972196e-01 1.77801157e-05]\n",
      " [1.03360929e-22 1.00878458e-10 5.40565588e-06 9.99994636e-01]\n",
      " [2.33881856e-08 9.98986304e-01 1.01366080e-03 8.76826078e-09]\n",
      " [3.40608554e-18 2.39523051e-05 1.14347629e-01 8.85628462e-01]\n",
      " [3.34405325e-29 2.83655710e-10 6.30017370e-04 9.99370039e-01]\n",
      " [1.56745796e-06 6.26944065e-01 3.72930616e-01 1.23749109e-04]\n",
      " [1.16560670e-27 1.94942586e-13 5.19083301e-08 1.00000000e+00]\n",
      " [9.76649344e-01 2.33506616e-02 1.31798570e-15 1.93406615e-19]\n",
      " [4.88100036e-16 2.04591226e-04 9.99116123e-01 6.79201563e-04]\n",
      " [9.32854147e-22 2.02856865e-09 8.34839375e-05 9.99916553e-01]\n",
      " [4.35739552e-04 9.99561846e-01 2.36620031e-06 4.41657555e-09]\n",
      " [3.61669332e-01 6.38330460e-01 2.38005114e-07 7.06161352e-10]\n",
      " [2.52808160e-22 7.61481829e-07 4.48825985e-01 5.51173210e-01]\n",
      " [8.34824278e-21 1.71187438e-08 2.32888991e-03 9.97671068e-01]\n",
      " [9.99965310e-01 3.46487577e-05 9.42479250e-21 1.33477335e-21]\n",
      " [2.06338354e-05 9.99942422e-01 3.69970548e-05 7.61894992e-09]\n",
      " [3.15657945e-17 1.15340322e-06 7.09971040e-03 9.92899179e-01]\n",
      " [6.92348635e-07 8.67252827e-01 1.32714033e-01 3.23456079e-05]\n",
      " [1.58317109e-12 2.38660621e-04 9.51812267e-01 4.79491204e-02]\n",
      " [8.59905429e-20 3.36476376e-07 4.57303599e-03 9.95426714e-01]\n",
      " [9.73161101e-01 2.68388223e-02 5.60953869e-16 1.22942903e-18]\n",
      " [2.71999387e-29 4.52511184e-13 3.86924967e-07 9.99999642e-01]\n",
      " [9.99409318e-01 5.90724521e-04 1.18597527e-18 3.71436407e-21]\n",
      " [9.99996066e-01 3.97280155e-06 3.41092670e-26 2.66307530e-28]\n",
      " [9.99961376e-01 3.85825406e-05 9.51240349e-24 8.12930839e-26]\n",
      " [2.07121742e-22 9.79030457e-09 5.47017145e-04 9.99453008e-01]\n",
      " [9.30983706e-17 5.45582261e-05 9.65518236e-01 3.44272368e-02]\n",
      " [7.01920840e-07 9.94834781e-01 5.16440161e-03 8.37621670e-08]\n",
      " [4.17363019e-11 3.62375110e-01 6.37615979e-01 8.92646767e-06]\n",
      " [8.38182635e-23 7.98518252e-10 1.04999148e-04 9.99894977e-01]\n",
      " [4.58695268e-07 9.99884963e-01 1.14603696e-04 2.49612175e-09]\n",
      " [1.24093442e-19 1.47744106e-06 8.88518095e-02 9.11146700e-01]\n",
      " [8.19069002e-15 9.10531453e-05 8.67700040e-01 1.32209003e-01]\n",
      " [9.98066127e-01 1.93388737e-03 3.92601533e-14 6.02967762e-16]\n",
      " [9.89288330e-01 1.07116206e-02 1.05152237e-15 5.94154321e-18]\n",
      " [2.02717670e-26 1.21824588e-12 1.34667943e-07 9.99999881e-01]\n",
      " [9.99729335e-01 2.70685821e-04 3.84987977e-22 5.10736062e-25]\n",
      " [2.76099601e-15 2.66416202e-04 9.99268591e-01 4.65025078e-04]\n",
      " [9.13693123e-21 3.19286210e-06 9.64193642e-01 3.58032025e-02]\n",
      " [9.31932633e-20 1.84807448e-06 6.40841350e-02 9.35914040e-01]\n",
      " [9.99998450e-01 1.54088661e-06 4.54595533e-24 2.53663080e-25]\n",
      " [1.68285050e-14 7.90481572e-04 9.99183834e-01 2.57265838e-05]\n",
      " [2.57654471e-11 3.12719075e-03 9.96230423e-01 6.42380328e-04]\n",
      " [3.47985529e-15 1.28442945e-03 9.98696744e-01 1.87812984e-05]\n",
      " [1.17502725e-02 9.88024771e-01 2.24487143e-04 4.67467032e-07]\n",
      " [2.83757736e-22 6.17243677e-06 8.00763130e-01 1.99230686e-01]\n",
      " [9.92203474e-01 7.79657392e-03 3.46301323e-14 1.17935838e-16]\n",
      " [9.09543633e-01 9.04564038e-02 4.11571764e-13 5.31776178e-16]\n",
      " [9.99809206e-01 1.90830819e-04 2.08336278e-20 6.88535847e-23]\n",
      " [1.24487579e-01 8.75512481e-01 9.68530367e-10 1.06442057e-13]\n",
      " [4.67595009e-06 9.95346248e-01 4.64864401e-03 4.31149090e-07]\n",
      " [1.21283610e-18 3.59569128e-07 4.39170096e-03 9.95607913e-01]\n",
      " [9.99999046e-01 1.01232035e-06 3.23610650e-26 5.32093758e-29]\n",
      " [4.77505836e-23 1.57582480e-08 7.91977346e-03 9.92080271e-01]\n",
      " [9.24477855e-12 7.95747736e-04 9.91863370e-01 7.34093366e-03]\n",
      " [1.31342020e-07 1.98298112e-01 8.01520288e-01 1.81436102e-04]\n",
      " [9.20908017e-10 2.03565970e-01 7.96408713e-01 2.53225971e-05]\n",
      " [4.76399767e-24 9.33148489e-11 4.37631888e-06 9.99995589e-01]\n",
      " [2.09414472e-20 1.94011154e-05 9.93938446e-01 6.04208140e-03]\n",
      " [3.09049729e-14 3.74394003e-04 9.99253809e-01 3.71735980e-04]\n",
      " [9.99231696e-01 7.68338039e-04 1.38065071e-20 1.08365103e-22]\n",
      " [9.99990582e-01 9.36129527e-06 3.12072011e-22 1.06376970e-23]\n",
      " [9.98586655e-01 1.41339866e-03 6.58089649e-16 4.87844614e-18]\n",
      " [6.47618234e-01 3.52381736e-01 2.29411198e-10 2.36780268e-13]\n",
      " [2.97739966e-21 6.55276290e-06 9.82545853e-01 1.74475349e-02]\n",
      " [8.05578311e-04 9.99103844e-01 9.04300250e-05 1.18413688e-07]\n",
      " [5.83458302e-07 9.92713034e-01 7.28578819e-03 6.10059999e-07]\n",
      " [8.52383891e-06 9.77049291e-01 2.29400955e-02 2.14172746e-06]\n",
      " [1.74725304e-21 6.90515378e-07 2.49415293e-01 7.50584006e-01]\n",
      " [1.72381789e-07 9.75315750e-01 2.46785935e-02 5.58907323e-06]\n",
      " [2.41405610e-03 9.97585773e-01 1.13315245e-07 4.44581004e-11]\n",
      " [5.72516680e-01 4.27483290e-01 6.75291822e-10 1.29174189e-12]\n",
      " [7.94427968e-10 1.11479228e-02 9.88776624e-01 7.53935092e-05]\n",
      " [3.40439482e-14 2.52293539e-04 9.99604881e-01 1.42907928e-04]\n",
      " [6.64860961e-14 6.61808881e-04 9.90028799e-01 9.30929743e-03]\n",
      " [1.67602807e-11 6.86373264e-02 9.31360126e-01 2.54637985e-06]\n",
      " [2.43251558e-31 4.27706156e-13 9.45660929e-07 9.99999046e-01]\n",
      " [5.35553691e-15 4.13677015e-04 9.51128364e-01 4.84579988e-02]\n",
      " [9.36091482e-12 2.94021964e-01 7.05977798e-01 2.91819646e-07]\n",
      " [1.00499302e-20 3.75953739e-08 1.16157974e-03 9.98838365e-01]\n",
      " [4.56519835e-32 1.00466137e-14 9.16518204e-08 9.99999881e-01]\n",
      " [2.41542726e-25 2.57982680e-09 1.89444609e-03 9.98105526e-01]\n",
      " [9.98230040e-01 1.76995667e-03 1.38883651e-16 1.98867082e-18]\n",
      " [7.23591516e-30 7.30336527e-13 4.26699302e-07 9.99999523e-01]\n",
      " [1.78236722e-18 9.17427133e-06 2.13929564e-01 7.86061227e-01]\n",
      " [9.99998331e-01 1.71842078e-06 2.26900836e-28 2.70183972e-30]\n",
      " [7.59951042e-30 1.19749427e-11 3.05498288e-05 9.99969482e-01]\n",
      " [2.94633979e-19 8.53464371e-06 3.89601260e-01 6.10390186e-01]\n",
      " [1.46265351e-22 4.31298508e-09 2.52544996e-04 9.99747455e-01]\n",
      " [9.04093977e-05 9.99883294e-01 2.63996335e-05 9.70673320e-10]\n",
      " [9.98653769e-01 1.34624366e-03 2.98535576e-15 8.30215042e-17]\n",
      " [5.16179485e-26 4.53735279e-11 4.84214070e-05 9.99951601e-01]\n",
      " [6.04072047e-05 9.99937057e-01 2.48121273e-06 2.51252352e-11]\n",
      " [4.26090099e-02 9.57388997e-01 1.91562071e-06 9.95285632e-09]\n",
      " [9.52016271e-05 9.99556005e-01 3.48656118e-04 8.93743461e-08]\n",
      " [7.85278290e-28 2.42425914e-12 3.67881682e-07 9.99999642e-01]\n",
      " [4.60574956e-09 6.60858154e-02 9.33846951e-01 6.72339229e-05]\n",
      " [2.80093660e-22 9.09655427e-08 2.27777124e-03 9.97722089e-01]\n",
      " [1.57026384e-19 2.29006146e-05 9.65684175e-01 3.42928432e-02]\n",
      " [1.18640426e-28 5.08118617e-11 4.85944074e-05 9.99951363e-01]\n",
      " [6.78803306e-04 9.99320984e-01 1.91816511e-07 6.11718384e-11]\n",
      " [4.61245421e-04 9.99504924e-01 3.38058926e-05 4.42997417e-09]\n",
      " [1.52043897e-24 7.81919307e-10 2.35396714e-04 9.99764621e-01]\n",
      " [1.51795790e-18 1.08093282e-05 3.22450399e-01 6.77538812e-01]\n",
      " [2.15200367e-15 2.44362163e-03 9.97554004e-01 2.40154623e-06]\n",
      " [4.11409622e-12 6.64901435e-02 9.33504820e-01 5.00593160e-06]\n",
      " [9.99995232e-01 4.71567319e-06 1.66162974e-25 7.59953997e-28]\n",
      " [9.99944329e-01 5.56840532e-05 1.64858130e-20 1.02994424e-22]\n",
      " [2.70863311e-18 1.61017742e-05 3.63060594e-01 6.36923254e-01]\n",
      " [7.48158693e-01 2.51841366e-01 1.87676055e-11 5.94402931e-14]\n",
      " [9.87150395e-18 3.32719828e-05 7.91949749e-01 2.08017066e-01]\n",
      " [4.38067307e-32 9.37044174e-15 2.34488766e-08 1.00000000e+00]\n",
      " [9.56482708e-01 4.35172170e-02 2.96511744e-14 5.15235749e-18]\n",
      " [3.45548858e-28 1.72116554e-09 4.21945564e-02 9.57805514e-01]\n",
      " [1.63050688e-21 7.60229213e-09 2.91233388e-04 9.99708712e-01]\n",
      " [1.17930787e-11 3.26698180e-03 9.96005595e-01 7.27426319e-04]\n",
      " [4.04440499e-07 9.56450701e-01 4.35454249e-02 3.39257417e-06]\n",
      " [6.48966480e-33 6.37788450e-15 9.71109699e-08 9.99999881e-01]\n",
      " [9.44663024e-16 2.27102864e-04 9.99442875e-01 3.30010400e-04]\n",
      " [1.88154027e-29 1.31082254e-12 1.35808614e-06 9.99998689e-01]\n",
      " [3.55969277e-18 7.96062668e-05 9.99919534e-01 8.70750569e-07]\n",
      " [2.84239403e-26 1.92248342e-10 1.62864322e-04 9.99837160e-01]\n",
      " [2.12858200e-01 7.87141800e-01 2.68290862e-10 1.36047044e-13]\n",
      " [9.91891742e-01 8.10828712e-03 2.43566526e-15 5.95724414e-19]\n",
      " [3.06957522e-31 1.14313493e-13 3.52215238e-07 9.99999642e-01]\n",
      " [5.14996111e-01 4.85003948e-01 7.72282466e-13 5.72557162e-16]\n",
      " [3.73495732e-06 9.99996185e-01 1.17998852e-07 6.59883953e-12]\n",
      " [5.46258538e-10 1.23768628e-01 8.76177073e-01 5.42954112e-05]\n",
      " [7.83699586e-25 5.10386577e-09 1.17061590e-03 9.98829424e-01]\n",
      " [8.40020039e-06 9.94579673e-01 5.41133806e-03 5.91261028e-07]\n",
      " [9.99982715e-01 1.73089502e-05 5.62162200e-25 1.32463747e-27]\n",
      " [1.38455187e-06 9.96973038e-01 3.02547938e-03 5.72557255e-08]\n",
      " [9.90423322e-01 9.57671553e-03 8.41687587e-16 2.53065548e-18]\n",
      " [3.63646851e-12 1.87131297e-03 9.53036785e-01 4.50919345e-02]\n",
      " [4.21140064e-03 9.95788634e-01 3.58425334e-09 4.56375996e-12]]\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "#Converting predictions to label\n",
    "pred = list()\n",
    "for i in range(len(y_pred)):\n",
    "    pred.append(np.argmax(y_pred[i]))\n",
    "#Converting one hot encoded test label to label\n",
    "test = list()\n",
    "for i in range(len(y_test)):\n",
    "    test.append(np.argmax(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 93.5%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "a = accuracy_score(pred, test)\n",
    "print('Accuracy is '+str(a*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use test data as validation data and can check the accuracies after every epoch. This will give us an insight into overfitting at the time of training only and we can take steps before the completion of all epochs. We can do this by changing fit function as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/100\n",
      "1800/1800 [==============================] - 0s 74us/step - loss: 0.0121 - val_loss: 0.1537\n",
      "Epoch 2/100\n",
      "1800/1800 [==============================] - 0s 58us/step - loss: 0.0116 - val_loss: 0.1480\n",
      "Epoch 3/100\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.0112 - val_loss: 0.1497\n",
      "Epoch 4/100\n",
      "1800/1800 [==============================] - 0s 51us/step - loss: 0.0111 - val_loss: 0.1493\n",
      "Epoch 5/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0108 - val_loss: 0.1515\n",
      "Epoch 6/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0106 - val_loss: 0.1528\n",
      "Epoch 7/100\n",
      "1800/1800 [==============================] - 0s 62us/step - loss: 0.0106 - val_loss: 0.1482\n",
      "Epoch 8/100\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.0105 - val_loss: 0.1500\n",
      "Epoch 9/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0102 - val_loss: 0.1536\n",
      "Epoch 10/100\n",
      "1800/1800 [==============================] - 0s 56us/step - loss: 0.0109 - val_loss: 0.1480\n",
      "Epoch 11/100\n",
      "1800/1800 [==============================] - 0s 49us/step - loss: 0.0097 - val_loss: 0.1489\n",
      "Epoch 12/100\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.0095 - val_loss: 0.1460\n",
      "Epoch 13/100\n",
      "1800/1800 [==============================] - 0s 41us/step - loss: 0.0094 - val_loss: 0.1491\n",
      "Epoch 14/100\n",
      "1800/1800 [==============================] - 0s 58us/step - loss: 0.0093 - val_loss: 0.1493\n",
      "Epoch 15/100\n",
      "1800/1800 [==============================] - 0s 78us/step - loss: 0.0092 - val_loss: 0.1484\n",
      "Epoch 16/100\n",
      "1800/1800 [==============================] - 0s 45us/step - loss: 0.0090 - val_loss: 0.1515\n",
      "Epoch 17/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0090 - val_loss: 0.1460\n",
      "Epoch 18/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0087 - val_loss: 0.1521\n",
      "Epoch 19/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.0087 - val_loss: 0.1418\n",
      "Epoch 20/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0084 - val_loss: 0.1512\n",
      "Epoch 21/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0087 - val_loss: 0.1501\n",
      "Epoch 22/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0082 - val_loss: 0.1468\n",
      "Epoch 23/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0080 - val_loss: 0.1468\n",
      "Epoch 24/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0079 - val_loss: 0.1455\n",
      "Epoch 25/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0078 - val_loss: 0.1476\n",
      "Epoch 26/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0076 - val_loss: 0.1499\n",
      "Epoch 27/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0077 - val_loss: 0.1485\n",
      "Epoch 28/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0078 - val_loss: 0.1489\n",
      "Epoch 29/100\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.0074 - val_loss: 0.1497\n",
      "Epoch 30/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0072 - val_loss: 0.1484\n",
      "Epoch 31/100\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.0071 - val_loss: 0.1444\n",
      "Epoch 32/100\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.0071 - val_loss: 0.1485\n",
      "Epoch 33/100\n",
      "1800/1800 [==============================] - 0s 50us/step - loss: 0.0069 - val_loss: 0.1501\n",
      "Epoch 34/100\n",
      "1800/1800 [==============================] - 0s 37us/step - loss: 0.0068 - val_loss: 0.1486\n",
      "Epoch 35/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0069 - val_loss: 0.1458\n",
      "Epoch 36/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0066 - val_loss: 0.1473\n",
      "Epoch 37/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0065 - val_loss: 0.1491\n",
      "Epoch 38/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0065 - val_loss: 0.1450\n",
      "Epoch 39/100\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.0062 - val_loss: 0.1464\n",
      "Epoch 40/100\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.0063 - val_loss: 0.1526\n",
      "Epoch 41/100\n",
      "1800/1800 [==============================] - 0s 27us/step - loss: 0.0061 - val_loss: 0.1439\n",
      "Epoch 42/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0060 - val_loss: 0.1544\n",
      "Epoch 43/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0060 - val_loss: 0.1515\n",
      "Epoch 44/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0059 - val_loss: 0.1483\n",
      "Epoch 45/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0059 - val_loss: 0.1480\n",
      "Epoch 46/100\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.0059 - val_loss: 0.1487\n",
      "Epoch 47/100\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.0056 - val_loss: 0.1487\n",
      "Epoch 48/100\n",
      "1800/1800 [==============================] - 0s 47us/step - loss: 0.0057 - val_loss: 0.1512\n",
      "Epoch 49/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0055 - val_loss: 0.1504\n",
      "Epoch 50/100\n",
      "1800/1800 [==============================] - 0s 64us/step - loss: 0.0056 - val_loss: 0.1591\n",
      "Epoch 51/100\n",
      "1800/1800 [==============================] - 0s 76us/step - loss: 0.0054 - val_loss: 0.1525\n",
      "Epoch 52/100\n",
      "1800/1800 [==============================] - 0s 34us/step - loss: 0.0053 - val_loss: 0.1547\n",
      "Epoch 53/100\n",
      "1800/1800 [==============================] - 0s 53us/step - loss: 0.0059 - val_loss: 0.1586\n",
      "Epoch 54/100\n",
      "1800/1800 [==============================] - 0s 50us/step - loss: 0.0054 - val_loss: 0.1467\n",
      "Epoch 55/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0052 - val_loss: 0.1575\n",
      "Epoch 56/100\n",
      "1800/1800 [==============================] - 0s 55us/step - loss: 0.0049 - val_loss: 0.1550\n",
      "Epoch 57/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0050 - val_loss: 0.1513\n",
      "Epoch 58/100\n",
      "1800/1800 [==============================] - 0s 59us/step - loss: 0.0049 - val_loss: 0.1454\n",
      "Epoch 59/100\n",
      "1800/1800 [==============================] - 0s 75us/step - loss: 0.0049 - val_loss: 0.1501\n",
      "Epoch 60/100\n",
      "1800/1800 [==============================] - 0s 58us/step - loss: 0.0047 - val_loss: 0.1518\n",
      "Epoch 61/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0046 - val_loss: 0.1561\n",
      "Epoch 62/100\n",
      "1800/1800 [==============================] - 0s 43us/step - loss: 0.0045 - val_loss: 0.1499\n",
      "Epoch 63/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.0054 - val_loss: 0.1647\n",
      "Epoch 64/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0047 - val_loss: 0.1512\n",
      "Epoch 65/100\n",
      "1800/1800 [==============================] - 0s 45us/step - loss: 0.0045 - val_loss: 0.1512\n",
      "Epoch 66/100\n",
      "1800/1800 [==============================] - 0s 43us/step - loss: 0.0043 - val_loss: 0.1528\n",
      "Epoch 67/100\n",
      "1800/1800 [==============================] - 0s 38us/step - loss: 0.0051 - val_loss: 0.1417\n",
      "Epoch 68/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0044 - val_loss: 0.1525\n",
      "Epoch 69/100\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.0041 - val_loss: 0.1503\n",
      "Epoch 70/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0039 - val_loss: 0.1534\n",
      "Epoch 71/100\n",
      "1800/1800 [==============================] - 0s 48us/step - loss: 0.0039 - val_loss: 0.1517\n",
      "Epoch 72/100\n",
      "1800/1800 [==============================] - 0s 44us/step - loss: 0.0039 - val_loss: 0.1539\n",
      "Epoch 73/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0038 - val_loss: 0.1550\n",
      "Epoch 74/100\n",
      "1800/1800 [==============================] - 0s 30us/step - loss: 0.0039 - val_loss: 0.1547\n",
      "Epoch 75/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0048 - val_loss: 0.1862\n",
      "Epoch 76/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.0044 - val_loss: 0.1569\n",
      "Epoch 77/100\n",
      "1800/1800 [==============================] - 0s 33us/step - loss: 0.0040 - val_loss: 0.1488\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800/1800 [==============================] - 0s 61us/step - loss: 0.0036 - val_loss: 0.1533\n",
      "Epoch 79/100\n",
      "1800/1800 [==============================] - 0s 39us/step - loss: 0.0036 - val_loss: 0.1517\n",
      "Epoch 80/100\n",
      "1800/1800 [==============================] - 0s 42us/step - loss: 0.0035 - val_loss: 0.1528\n",
      "Epoch 81/100\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.0034 - val_loss: 0.1559\n",
      "Epoch 82/100\n",
      "1800/1800 [==============================] - 0s 47us/step - loss: 0.0037 - val_loss: 0.1587\n",
      "Epoch 83/100\n",
      "1800/1800 [==============================] - 0s 102us/step - loss: 0.0033 - val_loss: 0.1548\n",
      "Epoch 84/100\n",
      "1800/1800 [==============================] - 0s 35us/step - loss: 0.0032 - val_loss: 0.1579\n",
      "Epoch 85/100\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.0031 - val_loss: 0.1567\n",
      "Epoch 86/100\n",
      "1800/1800 [==============================] - 0s 36us/step - loss: 0.0031 - val_loss: 0.1574\n",
      "Epoch 87/100\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.0031 - val_loss: 0.1604\n",
      "Epoch 88/100\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.0030 - val_loss: 0.1583\n",
      "Epoch 89/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.0032 - val_loss: 0.1604\n",
      "Epoch 90/100\n",
      "1800/1800 [==============================] - 0s 40us/step - loss: 0.0031 - val_loss: 0.1524\n",
      "Epoch 91/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0029 - val_loss: 0.1674\n",
      "Epoch 92/100\n",
      "1800/1800 [==============================] - 0s 24us/step - loss: 0.0030 - val_loss: 0.1558\n",
      "Epoch 93/100\n",
      "1800/1800 [==============================] - 0s 32us/step - loss: 0.0029 - val_loss: 0.1603\n",
      "Epoch 94/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0030 - val_loss: 0.1583\n",
      "Epoch 95/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0028 - val_loss: 0.1519\n",
      "Epoch 96/100\n",
      "1800/1800 [==============================] - 0s 28us/step - loss: 0.0028 - val_loss: 0.1511\n",
      "Epoch 97/100\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.0028 - val_loss: 0.1511\n",
      "Epoch 98/100\n",
      "1800/1800 [==============================] - 0s 26us/step - loss: 0.0027 - val_loss: 0.1480\n",
      "Epoch 99/100\n",
      "1800/1800 [==============================] - 0s 29us/step - loss: 0.0026 - val_loss: 0.1522\n",
      "Epoch 100/100\n",
      "1800/1800 [==============================] - 0s 31us/step - loss: 0.0026 - val_loss: 0.1533\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 100, validation_data = (X_test, y_test), batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is working fine. Now we will visualize training and validation losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
